{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"I love this product\",\n",
    "    \"This product is the best\",\n",
    "    \"It is a fantastic product\",\n",
    "    \"This is a bad product\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Sentence Embeddings:\n",
      " [array([0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1.]), array([0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.]), array([1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.]), array([1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1.])]\n"
     ]
    }
   ],
   "source": [
    "def Embedding(inputs):\n",
    "    tokenized_sequences = [word_tokenize(sentence.lower()) for sentence in inputs]\n",
    "\n",
    "    all_words = sorted(set(word for seq in tokenized_sequences for word in seq))\n",
    "    word_to_index = {word: i for i, word in enumerate(all_words, start=0)}  \n",
    "    vocab_size = len(word_to_index)  \n",
    "\n",
    "    one_hot_encoded = []\n",
    "    for seq in tokenized_sequences:\n",
    "        sentence_vector = np.zeros(vocab_size) \n",
    "        for word in seq:\n",
    "            if word in word_to_index:\n",
    "                index = word_to_index[word]\n",
    "                sentence_vector[index] = 1  \n",
    "        one_hot_encoded.append(sentence_vector)\n",
    "\n",
    "\n",
    "\n",
    "    return one_hot_encoded\n",
    "\n",
    "sentence_embeddings = Embedding(inputs)\n",
    "print(\"\\nFinal Sentence Embeddings:\\n\", sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1.]),\n",
       " array([0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.]),\n",
       " array([1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.]),\n",
       " array([1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1.])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimpleRNN(inputs):\n",
    "    sentence_embeddings = Embedding(inputs)\n",
    "    sentence_embeddings = np.array(sentence_embeddings)\n",
    "    print(\"Sentence Embeddings Shape:\", sentence_embeddings.shape)\n",
    "\n",
    "    input_size = sentence_embeddings.shape[1]  # dynamically get input size\n",
    "    hidden_size = 3\n",
    "    output_size = 3\n",
    "\n",
    "    # Create RNN weights\n",
    "    W = np.random.rand(hidden_size, input_size)  # (3, input_size)\n",
    "    U = np.random.rand(hidden_size, hidden_size)  # (3, 3)\n",
    "    V = np.random.rand(output_size, hidden_size)  # (3, 3)\n",
    "\n",
    "    h = np.zeros(hidden_size)\n",
    "    y = []\n",
    "\n",
    "    for s in sentence_embeddings:\n",
    "        h = np.tanh(np.dot(W, s) + np.dot(U, h))  # (3,)\n",
    "        o = np.dot(V, h)  # (3,)\n",
    "        y.append(o)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1.]),\n",
       " array([0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.]),\n",
       " array([1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.]),\n",
       " array([1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1.])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings = Embedding(inputs)\n",
    "sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embeddings Shape: (4, 11)\n"
     ]
    }
   ],
   "source": [
    "output = SimpleRNN(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.32693828, 2.11080631, 1.48054214]),\n",
       " array([0.3444898 , 2.27576716, 1.58157489]),\n",
       " array([0.34471151, 2.27366483, 1.57912069]),\n",
       " array([0.34447979, 2.27293131, 1.57844277])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
