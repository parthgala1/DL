{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad69f4fd-2bf0-4c62-bdc6-dc5b82a44ceb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (947603589.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython3.10 -m venv tf-env\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b11247df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cf73b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exp 1- 8 as per executed in labs Inputs will be varied practicals hard coded , some with tensorflow and keras CNN, rnn, autoencoder and backpropogation can be implemented on datasets mostly mnist, iris, catand dog Sometimes u need to take inputs of dataset by creating shapes So there are variations while taking inputs Instructions while coming to the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168853d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d070f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Delta_b:  [-1, 0, 0, 1] Final Weights:  [1, 1] Final Bias:  0\n",
      "Epoch:  2 Delta_b:  [-1, -1, 0, 1] Final Weights:  [1, 2] Final Bias:  -1\n",
      "Epoch:  3 Delta_b:  [0, -1, -1, 1] Final Weights:  [1, 2] Final Bias:  -2\n",
      "Epoch:  4 Delta_b:  [0, 0, -1, 1] Final Weights:  [2, 2] Final Bias:  -2\n",
      "Epoch:  5 Delta_b:  [0, -1, 0, 0] Final Weights:  [1, 2] Final Bias:  -3\n",
      "Epoch:  6 Delta_b:  [0, 0, 0, 0] Final Weights:  [1, 2] Final Bias:  -3\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: Perceptron Learning Algorithm\n",
    "\n",
    "def bipolar(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    elif x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def unipolar(net):\n",
    "    if net < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def process(x, d, w1, w2, b, activation, delta_b, c=1):\n",
    "    net = w1*x[0] + w2*x[1] + b\n",
    "    fnet = activation(net)\n",
    "    error = d - fnet\n",
    "    del_w1 = c * error * x[0]\n",
    "    del_w2 = c * error * x[1]\n",
    "    del_b = error\n",
    "    delta_b.append(del_b)\n",
    "    w1 += del_w1\n",
    "    w2 += del_w2\n",
    "    b += del_b\n",
    "    return w1, w2, b, delta_b\n",
    "\n",
    "x = [[0, 0], \n",
    "     [1, 0],\n",
    "     [0, 1],\n",
    "     [1, 1]]\n",
    "d = [0, 0, 0, 1]\n",
    "w1, w2, b = 0, 0, 0\n",
    "epoch = 0\n",
    "delta_b = []\n",
    "while delta_b != [0, 0, 0, 0]:\n",
    "    epoch += 1\n",
    "    delta_b = []\n",
    "    for i in range(len(x)):\n",
    "        w1, w2, b, delta_b = process(x[i], d[i], w1, w2, b, unipolar, delta_b)\n",
    "\n",
    "    print(\"Epoch: \", epoch, \"Delta_b: \", delta_b, \"Final Weights: \", [w1, w2], \"Final Bias: \", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2068374f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782644e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Experiment 2: Packpropagation Algorithm\n",
    "# Forward and Backward Pass\n",
    "\n",
    "# 0 - 0 - 0\n",
    "# 0 - 0 - 0\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def forward_pass(input, weight, bias, activation):\n",
    "    layer = np.dot(weight, input) + bias\n",
    "    return float(activation(layer)[0])\n",
    "\n",
    "def error_calculation(excepted_output, output):\n",
    "    return 0.5 * np.sum((excepted_output - output) ** 2)\n",
    "\n",
    "def update_weights_inner(output, target, hidden, input, weight_affecting, weight_i, learning_rate):\n",
    "    error_i_1 = (output[0] - target[0]) * (output[0] * (1 - output[0])) * weight_affecting[0] * (hidden[0] * (1 - hidden[0])) * input\n",
    "    error_i_2 = (output[1] - target[1]) * (output[1] * (1 - output[1])) * weight_affecting[1] * (hidden[0] * (1 - hidden[0])) * input\n",
    "    error_i = error_i_1 + error_i_2\n",
    "    weight = weight_i - learning_rate * error_i\n",
    "    return weight\n",
    "\n",
    "def update_weights_outer(output, target, input, weight_i, learning_rate):\n",
    "    error_i = (output - target) * (output * (1 - output)) * input\n",
    "    weight = weight_i - learning_rate * error_i\n",
    "    return weight\n",
    "\n",
    "def backward_pass(weights, biases, inputs, hidden, excepted_output, output):\n",
    "    w5 = update_weights_outer(output[0], excepted_output[0], hidden[0], weights[4], 0.6)\n",
    "    w6 = update_weights_outer(output[0], excepted_output[0], hidden[1], weights[5], 0.6)\n",
    "    w7 = update_weights_outer(output[1], excepted_output[1], hidden[1], weights[6], 0.6)\n",
    "    w8 = update_weights_outer(output[0], excepted_output[0], hidden[1], weights[7], 0.6)\n",
    "\n",
    "    w1 = update_weights_inner(output, excepted_output, hidden, inputs[0], [weights[4], weights[6]], weights[0], 0.6)\n",
    "    w2 = update_weights_inner(output, excepted_output, hidden, inputs[1], [weights[5], weights[7]], weights[2], 0.6)\n",
    "    w3 = update_weights_inner(output, excepted_output, hidden, inputs[0], [weights[4], weights[6]], weights[1], 0.6)\n",
    "    w4 = update_weights_inner(output, excepted_output, hidden, inputs[1], [weights[5], weights[7]], weights[3], 0.6)\n",
    "\n",
    "    return np.array([w1, w2, w3, w4, w5, w6, w7, w8])\n",
    "\n",
    "inputs = [0.1, 0.5]\n",
    "weights = [0.1, 0.3, 0.2, 0.4 , 0.5, 0.6, 0.7, 0.8]\n",
    "biases = [0.25, 0.25, 0.35, 0.35]\n",
    "excepted_output = [0.05, 0.95]\n",
    "\n",
    "hidden_network1 = forward_pass(inputs, weights[:2], biases[:1], sigmoid)\n",
    "hidden_network2 = forward_pass(inputs, weights[2:4], biases[1:2], sigmoid)\n",
    "hidden = [hidden_network1, hidden_network2]\n",
    "output_network1 = forward_pass(hidden, weights[4:6], biases[2:3], sigmoid)\n",
    "output_network2 = forward_pass(hidden, weights[6:8], biases[3:4], sigmoid)\n",
    "output = [output_network1, output_network2]\n",
    "\n",
    "error = error_calculation(np.array(excepted_output), np.array(output))\n",
    "\n",
    "# updated_weights = backward_pass(weights, biases, inputs, hidden, excepted_output, output)\n",
    "# print(\"Updated Weights: \", updated_weights)\n",
    "\n",
    "epochs = 47000\n",
    "\n",
    "for i in range(epochs):\n",
    "    hidden_network1 = forward_pass(inputs, weights[:2], biases[:1], sigmoid)\n",
    "    hidden_network2 = forward_pass(inputs, weights[2:4], biases[1:2], sigmoid)\n",
    "    hidden = [hidden_network1, hidden_network2]\n",
    "    output_network1 = forward_pass(hidden, weights[4:6], biases[2:3], sigmoid)\n",
    "    output_network2 = forward_pass(hidden, weights[6:8], biases[3:4], sigmoid)\n",
    "    output = [output_network1, output_network2]\n",
    "\n",
    "    error = error_calculation(np.array(excepted_output), np.array(output))\n",
    "\n",
    "    updated_weights = backward_pass(updated_weights, biases, inputs, hidden, excepted_output, output)\n",
    "    weights = updated_weights\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Epoch {i}: Error: {error}\")\n",
    "    if error < 0.01:\n",
    "        break\n",
    "print(\"Final Output: \", output)\n",
    "print(\"Final Weights: \", weights)\n",
    "print(\"Final Error: \", error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d0e903",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04adbd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: CNN\n",
    "\n",
    "def convolve2d(image, kernel, stride):\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "    image_height, image_width = image.shape\n",
    "    output_height = (image_height - kernel_height) // stride + 1\n",
    "    output_width = (image_width - kernel_width) // stride + 1\n",
    "    new_image = np.zeros((output_height, output_width)).astype(np.int32)\n",
    "    for x in range(0, output_width):\n",
    "        for y in range(0, output_height):\n",
    "            new_image[y][x] = np.sum(image[y * stride:y * stride + kernel_height, x * stride:x * stride + kernel_width] * kernel).astype(np.int32)\n",
    "    return new_image\n",
    "\n",
    "def maxpooling(input, pool_size, stride):\n",
    "    input_height, input_width = input.shape\n",
    "    output_height = (input_height - pool_size) // stride + 1\n",
    "    output_width = (input_width - pool_size) // stride + 1\n",
    "    new_image = np.zeros((output_height, output_width)).astype(np.int32)\n",
    "    for i in range(output_height):\n",
    "        for j in range(output_width):\n",
    "            new_image[i, j] = np.max(input[i:i + pool_size, j:j + pool_size])\n",
    "    return new_image.astype(np.int32)\n",
    "\n",
    "\n",
    "def flatten(input):\n",
    "    return input.flatten()\n",
    "\n",
    "def dense(input, weights, bias):\n",
    "    return np.dot(weights, input) + bias\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "def train_cnn(input, kernel, pool_size, stride):\n",
    "    conv_output = convolve2d(input, kernel, stride)\n",
    "    relu_output = relu(conv_output)\n",
    "    print(\"Convolution Output: \\n\", relu_output)\n",
    "    plt.imshow(relu_output, cmap='gray')\n",
    "    plt.title(\"Convolution Output\")\n",
    "    plt.show()\n",
    "\n",
    "    pool_output = maxpooling(relu_output, pool_size, stride)\n",
    "    print(\"Max Pooling Output: \\n\", pool_output)\n",
    "    plt.imshow(pool_output, cmap='gray')\n",
    "    plt.title(\"Max Pooling Output\")\n",
    "    plt.show()\n",
    "\n",
    "    flatten_output = flatten(pool_output)\n",
    "    print(\"Flatten Output: \\n\", flatten_output)\n",
    "    plt.imshow(flatten_output.reshape(1, -1), cmap='gray')\n",
    "    plt.title(\"Flatten Output\")\n",
    "    plt.show()\n",
    "\n",
    "    weights = np.random.rand(flatten_output.size)\n",
    "    bias = np.random.rand(1)\n",
    "    dense_output = dense(flatten_output, weights, bias)\n",
    "    print(\"Dense Layer Output: \\n\", dense_output)\n",
    "    plt.imshow(dense_output.reshape(1, -1), cmap='gray')\n",
    "    plt.title(\"Dense Layer Output\")\n",
    "    plt.show()\n",
    "\n",
    "    sigmoid_output = sigmoid(dense_output)\n",
    "    return sigmoid_output\n",
    "pool_size = 2\n",
    "stride = 1\n",
    "input = np.random.randint(0, 255, (9, 9)).astype(np.int32)\n",
    "kernel = np.array([[1, 3, 1], \n",
    "                   [2, 0, 0], \n",
    "                   [-1, 1, -2]])\n",
    "# [[-1, 0, 1],\n",
    "#  [-1, 0, 1],\n",
    "#  [-1, 0 ,1]]\n",
    "print(\"Input Image: \\n\", input)\n",
    "print(\"Kernel: \\n\", kernel)\n",
    "plt.imshow(input, cmap='gray')\n",
    "plt.title(\"Input Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "sigmoid_output = train_cnn(input, kernel, pool_size, stride)\n",
    "print(\"sigmoid Output: \", sigmoid_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7637ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8997 - loss: 0.3275 - val_accuracy: 0.9788 - val_loss: 0.0741\n",
      "Epoch 2/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.9818 - loss: 0.0597 - val_accuracy: 0.9799 - val_loss: 0.0694\n",
      "Epoch 3/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9895 - loss: 0.0339 - val_accuracy: 0.9848 - val_loss: 0.0518\n",
      "Epoch 4/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9923 - loss: 0.0244 - val_accuracy: 0.9854 - val_loss: 0.0532\n",
      "Epoch 5/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9962 - loss: 0.0136 - val_accuracy: 0.9858 - val_loss: 0.0550\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9804 - loss: 0.0608\n",
      "Test accuracy: 0.9847999811172485\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "data = mnist.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = data\n",
    "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19cb6b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5408</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">692,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5408\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m692,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,081,888</span> (7.94 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,081,888\u001b[0m (7.94 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">693,962</span> (2.65 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m693,962\u001b[0m (2.65 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,387,926</span> (5.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,387,926\u001b[0m (5.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77c8788",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "predicted_classes = np.argmax(y_pred, axis=1)\n",
    "print(\"Predicted classes: \", predicted_classes)\n",
    "print(\"True classes: \", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267eeca1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Experiment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f405800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "RNN Output: \n",
      " [[0.99989174]\n",
      " [0.99992822]\n",
      " [0.99992822]\n",
      " [0.99992822]\n",
      " [0.99992822]]\n"
     ]
    }
   ],
   "source": [
    "# Experiment 4: RNN\n",
    "\n",
    "input_sentences = [\n",
    "    \"I love programming\",\n",
    "    \"Python is great\",\n",
    "    \"I enjoy learning new things\",\n",
    "    \"TensorFlow is a powerful library\",\n",
    "    \"Deep learning is fascinating\"\n",
    "]\n",
    "\n",
    "def one_hot_encode(sentences):\n",
    "    tokens = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "    vocab = set(word for sentence in tokens for word in sentence)\n",
    "    print(len(vocab))\n",
    "    word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "    encoded = []\n",
    "    for sentence in tokens:\n",
    "        encoded_sentence = np.zeros(len(vocab))\n",
    "        for word in sentence:\n",
    "            encoded_sentence[word_to_index[word]] = 1\n",
    "        encoded.append(encoded_sentence)\n",
    "    return np.array(encoded)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def SimpleRNN(input):\n",
    "    embeddings = one_hot_encode(input)\n",
    "\n",
    "    input_size = embeddings.shape[1]\n",
    "    hidden_size = 20\n",
    "    output_size = 1\n",
    "    weights_input = np.random.rand(input_size, hidden_size)\n",
    "    weights_hidden = np.random.rand(hidden_size, hidden_size)\n",
    "    weights_output = np.random.rand(hidden_size, output_size)\n",
    "    biases_hidden = np.random.rand(hidden_size)\n",
    "    biases_output = np.random.rand(output_size)\n",
    "    hidden_state = np.zeros(hidden_size)\n",
    "    outputs = []\n",
    "\n",
    "    for t in embeddings:\n",
    "        hidden_state = np.tanh(np.dot(t, weights_input) + np.dot(hidden_state, weights_hidden) + biases_hidden)\n",
    "        output = sigmoid(np.dot(hidden_state, weights_output) + biases_output)\n",
    "        outputs.append(output)\n",
    "    return np.array(outputs)\n",
    "\n",
    "rnn_output = SimpleRNN(input_sentences)\n",
    "print(\"RNN Output: \\n\", rnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fea500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 60ms/step - accuracy: 0.5058 - loss: 0.6953 - val_accuracy: 0.5586 - val_loss: 0.6907\n",
      "Epoch 2/5\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5427 - loss: 0.6852"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "data = imdb.load_data(num_words=1000)\n",
    "(x_train, y_train),(x_test, y_test) = data\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "model.add(layers.SimpleRNN(128))\n",
    "model.add(layers.Dense(10, activation='sigmoid'))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=500)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=500)\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d831d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "predicted_classes = np.argmax(y_pred, axis=1)\n",
    "print(\"Predicted classes: \", predicted_classes)\n",
    "print(\"True classes: \", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91681df0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Experiment 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e840f8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.38)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([0.2, 0.4])\n",
    "w = np.array([[0.1,0.4],[0.2,0.6],[0.3,0.2],[0.4, 0.5]])\n",
    "a = 0.2\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "  d = np.sum((w-x)**2, axis = 1)\n",
    "  bmu = np.argmin(d)\n",
    "  bmu_d = d[bmu]\n",
    "  w[bmu] += a * (x - w[bmu])\n",
    "  a *= 0.5\n",
    "\n",
    "  print(f\"Epoch {epoch}: BMU = Y{bmu+1}, Distance = {bmu_d}\")\n",
    "  print(f\"Weights: \\n{w}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93319c4-505a-42f5-b2a8-2ac1546df22b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Experiment 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "827f89a5-f19b-424c-aa69-2193cab133c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoders:  0.7987500476528985\n",
      "PCA:  0.2845642321869495\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def circle(r, θ):\n",
    "    x = r * np.cos(θ)\n",
    "    y = r * np.sin(θ)\n",
    "    return x, y\n",
    "\n",
    "def data_generated():\n",
    "    train = []\n",
    "    for i in range(1000):\n",
    "        θ = np.random.uniform(0, 2*np.pi)\n",
    "        r = np.random.randn()\n",
    "        x, y = (circle(r, θ))\n",
    "        train.append([x, y])\n",
    "    return np.array(train)\n",
    "\n",
    "def PCA_train(train):\n",
    "    scale = StandardScaler()\n",
    "    scaled_train = scale.fit_transform(train)\n",
    "    pca = PCA(n_components=1)\n",
    "    train_pca = pca.fit_transform(scaled_train)\n",
    "    inversed = pca.inverse_transform(train_pca)\n",
    "\n",
    "    mse = ((train - inversed) ** 2).mean(axis=None)\n",
    "    return mse\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def encoder(data, weights, bias):\n",
    "    return tanh(np.dot(data, weights) + bias)\n",
    "\n",
    "def decoder(data, weights, bias):\n",
    "    return tanh(np.dot(data, weights) + bias)\n",
    "\n",
    "def autoencoders(encoder, decoder, data):\n",
    "    input_dim = data.shape[1]\n",
    "    latent_dim = 1\n",
    "    \n",
    "    weight_encoder = np.random.randn(input_dim, latent_dim)\n",
    "    bias_encoder = np.random.randn(latent_dim)\n",
    "    weight_decoder = np.random.randn(latent_dim, input_dim)\n",
    "    bias_decoder = np.random.randn(input_dim)\n",
    "\n",
    "    z1 = encoder(data, weight_encoder, bias_encoder)\n",
    "    z2 = decoder(z1, weight_decoder, bias_decoder)\n",
    "\n",
    "    mse = ((data - z2)** 2).mean()\n",
    "    return mse\n",
    "\n",
    "train = data_generated()\n",
    "\n",
    "ae = autoencoders(encoder, decoder, train)\n",
    "pca = PCA_train(train)\n",
    "\n",
    "print(\"Autoencoders: \", ae)\n",
    "print(\"PCA: \", pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88acd2df-da6e-4252-b251-37a0577bcde5",
   "metadata": {},
   "source": [
    "## Experiment 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1799a245-8cb7-4f67-ac13-0507ffcc7f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "    \n",
    "def encoder(data, weight, bias):\n",
    "    return relu(np.dot(data, weight) + bias)\n",
    "\n",
    "def decoder(data, weight, bias):\n",
    "    return relu(np.dot(data, weight) + bias)\n",
    "\n",
    "def vae(data, encoder, decoder):\n",
    "    input_dim = data.shape[0]\n",
    "    hidden_dim = 4\n",
    "    latent_dim = 2\n",
    "\n",
    "    w1 = np.random.randn(input_dim, hidden_dim)\n",
    "    b1 = np.random.randn(hidden_dim)\n",
    "\n",
    "    w_mu = np.random.randn(hidden_dim, latent_dim)\n",
    "    b_mu = np.random.randn(latent_dim)\n",
    "\n",
    "    w_logvar = np.random.randn(hidden_dim, latent_dim)\n",
    "    b_logvar = np.random.randn(latent_dim)\n",
    "\n",
    "    w3 = np.random.randn(latent_dim, hidden_dim)\n",
    "    b3 = np.random.randn(hidden_dim)\n",
    "\n",
    "    w4 = np.random.randn(hidden_dim, input_dim)\n",
    "    b4 = np.random.randn(input_dim)\n",
    "\n",
    "    # Forward pass\n",
    "\n",
    "    z1 = encoder(data, w1, b1)\n",
    "    mu = encoder(z1, w_mu, b_mu)\n",
    "    logvar = encoder(z1, w_logvar, b_logvar)\n",
    "\n",
    "    std = np.exp(0.5 * logvar)\n",
    "    epsilon = np.random.randn(*std.shape)\n",
    "    z = mu + epsilon * std\n",
    "\n",
    "    z3 = decoder(z, w3, b3)\n",
    "    x_hat = decoder(z3, w4, b4)\n",
    "\n",
    "    return x_hat\n",
    "\n",
    "data = load_digits()\n",
    "x, y = data.data, data.target\n",
    "for i in x[:2]:    \n",
    "    reconstructed = vae(i, encoder, decoder)\n",
    "    plt.figure(figsize=(5,10))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(i.reshape(8, 8), cmap=\"gray\")\n",
    "    \n",
    "    \n",
    "    plt.subplot(1, 2, 2) \n",
    "    plt.imshow(reconstructed.reshape(8, 8), cmap=\"gray\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775b6897-867b-4fa1-89cc-c78d5b935f03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Experiment 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "534d9240-f632-4bc6-bc24-a0a82d0d28e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "Epoch:  0 G_loss:  3.864848214050328 D_loss:  2.2502719212236157\n",
      "Epoch:  100 G_loss:  0.6860975100397663 D_loss:  1.4039427934053046\n",
      "Epoch:  200 G_loss:  0.6914498906079076 D_loss:  1.3914707127648032\n",
      "Epoch:  300 G_loss:  0.6795174440946532 D_loss:  1.4060387719163714\n",
      "Epoch:  400 G_loss:  0.7027738226362841 D_loss:  1.39319946546764\n",
      "Epoch:  500 G_loss:  0.7153165472228527 D_loss:  1.3859684718203353\n",
      "Epoch:  600 G_loss:  0.6977612891387567 D_loss:  1.3832682061213664\n",
      "Epoch:  700 G_loss:  0.6956553743103089 D_loss:  1.383922947017517\n",
      "Epoch:  800 G_loss:  0.683999380735203 D_loss:  1.402383023440236\n",
      "Epoch:  900 G_loss:  0.6838979337705752 D_loss:  1.3957968299464545\n",
      "Epoch:  1000 G_loss:  0.691932812472624 D_loss:  1.3886842287293688\n",
      "Epoch:  1100 G_loss:  0.6840202422101751 D_loss:  1.4092772240719815\n",
      "Epoch:  1200 G_loss:  0.6820170327625001 D_loss:  1.4076506729336449\n",
      "Epoch:  1300 G_loss:  0.6859233522431512 D_loss:  1.3969157513579438\n",
      "Epoch:  1400 G_loss:  0.6965899326701228 D_loss:  1.3891936510490213\n",
      "Epoch:  1500 G_loss:  0.7048994711325182 D_loss:  1.3896123948617847\n",
      "Epoch:  1600 G_loss:  0.6971406328037906 D_loss:  1.383313670083559\n",
      "Epoch:  1700 G_loss:  0.7269704134564469 D_loss:  1.3611170763321265\n",
      "Epoch:  1800 G_loss:  0.6990764984330251 D_loss:  1.3848281194394527\n",
      "Epoch:  1900 G_loss:  0.6888106744426961 D_loss:  1.3953249834521897\n",
      "Epoch:  2000 G_loss:  0.7609931483480069 D_loss:  1.3432400181077808\n",
      "Epoch:  2100 G_loss:  0.6945634685375905 D_loss:  1.3853547359910905\n",
      "Epoch:  2200 G_loss:  0.6768329713335903 D_loss:  1.4071319685373467\n",
      "Epoch:  2300 G_loss:  0.7011563178118491 D_loss:  1.380943236951222\n",
      "Epoch:  2400 G_loss:  0.697769572104662 D_loss:  1.420852588530868\n",
      "Epoch:  2500 G_loss:  0.7019873009213028 D_loss:  1.3869412921035456\n",
      "Epoch:  2600 G_loss:  0.7345439965071948 D_loss:  1.3528663303063901\n",
      "Epoch:  2700 G_loss:  0.6941172202521677 D_loss:  1.389150018380071\n",
      "Epoch:  2800 G_loss:  0.6814807703112982 D_loss:  1.4028119608172789\n",
      "Epoch:  2900 G_loss:  0.7094390606406269 D_loss:  1.3727029783405038\n",
      "Epoch:  3000 G_loss:  0.6917178689666797 D_loss:  1.3921309125544314\n",
      "Epoch:  3100 G_loss:  0.7026339844128718 D_loss:  1.378023024373923\n",
      "Epoch:  3200 G_loss:  0.6857640863401577 D_loss:  1.3967926586363646\n",
      "Epoch:  3300 G_loss:  0.6814991642779191 D_loss:  1.4006400767595542\n",
      "Epoch:  3400 G_loss:  0.6902098436480351 D_loss:  1.3919820724363634\n",
      "Epoch:  3500 G_loss:  0.6983296446905657 D_loss:  1.3903577599461183\n",
      "Epoch:  3600 G_loss:  0.7115586764150046 D_loss:  1.381569543428868\n",
      "Epoch:  3700 G_loss:  0.6884638333815449 D_loss:  1.3929145146310629\n",
      "Epoch:  3800 G_loss:  0.7141924600995673 D_loss:  1.3719106205203246\n",
      "Epoch:  3900 G_loss:  0.6467540488268954 D_loss:  1.4816352634446295\n",
      "Epoch:  4000 G_loss:  0.6874448625890124 D_loss:  1.3964978837949196\n",
      "Epoch:  4100 G_loss:  0.7049777209960975 D_loss:  1.3849753039668709\n",
      "Epoch:  4200 G_loss:  0.7016804760901446 D_loss:  1.4226357265681377\n",
      "Epoch:  4300 G_loss:  0.6837760103909873 D_loss:  1.3998830355862573\n",
      "Epoch:  4400 G_loss:  0.7212798918427326 D_loss:  1.3697379020245526\n",
      "Epoch:  4500 G_loss:  0.6969807058628592 D_loss:  1.3870764156371018\n",
      "Epoch:  4600 G_loss:  0.6835397101127694 D_loss:  1.4062141170503484\n",
      "Epoch:  4700 G_loss:  0.6951027271169177 D_loss:  1.386564212182215\n",
      "Epoch:  4800 G_loss:  0.695189103461753 D_loss:  1.3842863074595493\n",
      "Epoch:  4900 G_loss:  0.6950724877094597 D_loss:  1.3863776180732277\n",
      "Epoch:  5000 G_loss:  0.6958831725214152 D_loss:  1.3896118482830366\n",
      "Epoch:  5100 G_loss:  0.7006852356437836 D_loss:  1.3822373684673157\n",
      "Epoch:  5200 G_loss:  0.6820964019893454 D_loss:  1.399355155012972\n",
      "Epoch:  5300 G_loss:  0.6936457173064736 D_loss:  1.3937341453561962\n",
      "Epoch:  5400 G_loss:  0.6871794710084569 D_loss:  1.3922999683076314\n",
      "Epoch:  5500 G_loss:  0.6890381486906316 D_loss:  1.392223458185741\n",
      "Epoch:  5600 G_loss:  0.6912497278097853 D_loss:  1.3882818520369242\n",
      "Epoch:  5700 G_loss:  0.7212081851645205 D_loss:  1.3687049973069858\n",
      "Epoch:  5800 G_loss:  0.737237268389033 D_loss:  1.3556795123784227\n",
      "Epoch:  5900 G_loss:  0.6772925938383039 D_loss:  1.406452211498519\n",
      "Epoch:  6000 G_loss:  0.7123399096624834 D_loss:  1.3691192130979084\n",
      "Epoch:  6100 G_loss:  0.7018031078053071 D_loss:  1.377619031975547\n",
      "Epoch:  6200 G_loss:  0.6977686900360492 D_loss:  1.3959165265253721\n",
      "Epoch:  6300 G_loss:  0.6855113119888704 D_loss:  1.4060203791423755\n",
      "Epoch:  6400 G_loss:  0.6958737869808613 D_loss:  1.3839855667162126\n",
      "Epoch:  6500 G_loss:  0.7237944316847194 D_loss:  1.3687413150878183\n",
      "Epoch:  6600 G_loss:  0.6943303487457417 D_loss:  1.3869227181540666\n",
      "Epoch:  6700 G_loss:  0.7117406973015872 D_loss:  1.379100753572637\n",
      "Epoch:  6800 G_loss:  0.6706196028248943 D_loss:  1.4237571631321568\n",
      "Epoch:  6900 G_loss:  0.7264540304467102 D_loss:  1.3700458415889205\n",
      "Epoch:  7000 G_loss:  0.6972059282877998 D_loss:  1.386676778050595\n",
      "Epoch:  7100 G_loss:  0.7125834832226149 D_loss:  1.3691380054673108\n",
      "Epoch:  7200 G_loss:  0.7363441877355733 D_loss:  1.3515461400010989\n",
      "Epoch:  7300 G_loss:  0.7006208407120306 D_loss:  1.37995411298022\n",
      "Epoch:  7400 G_loss:  0.7036936974212167 D_loss:  1.3781982730812017\n",
      "Epoch:  7500 G_loss:  0.7153310359223843 D_loss:  1.3700630022670914\n",
      "Epoch:  7600 G_loss:  0.7166391111919375 D_loss:  1.3805093473178744\n",
      "Epoch:  7700 G_loss:  0.6890231801077378 D_loss:  1.4313406490743648\n",
      "Epoch:  7800 G_loss:  0.6908543860650995 D_loss:  1.3981725705598202\n",
      "Epoch:  7900 G_loss:  0.7428574247574864 D_loss:  1.3723250352066776\n",
      "Epoch:  8000 G_loss:  0.7225505284382284 D_loss:  1.37102818770157\n",
      "Epoch:  8100 G_loss:  0.6821636289910042 D_loss:  1.4177346638000625\n",
      "Epoch:  8200 G_loss:  0.7113368454173814 D_loss:  1.3689870872394418\n",
      "Epoch:  8300 G_loss:  0.6924086040788081 D_loss:  1.4012677352343457\n",
      "Epoch:  8400 G_loss:  0.6899179794510086 D_loss:  1.3936303791942113\n",
      "Epoch:  8500 G_loss:  0.6965369885290089 D_loss:  1.3850289811755687\n",
      "Epoch:  8600 G_loss:  0.6949165178202126 D_loss:  1.4076371031559582\n",
      "Epoch:  8700 G_loss:  0.6987715594834278 D_loss:  1.383355676861941\n",
      "Epoch:  8800 G_loss:  0.6931791095436255 D_loss:  1.3870206090707242\n",
      "Epoch:  8900 G_loss:  0.6723542877453568 D_loss:  1.4448140695668008\n",
      "Epoch:  9000 G_loss:  0.6882932293252163 D_loss:  1.3992028607316045\n",
      "Epoch:  9100 G_loss:  0.7122274338525382 D_loss:  1.376211246535518\n",
      "Epoch:  9200 G_loss:  0.6875543927454179 D_loss:  1.3990096256916655\n",
      "Epoch:  9300 G_loss:  0.6910748217920768 D_loss:  1.3969150945667073\n",
      "Epoch:  9400 G_loss:  0.7072270989469008 D_loss:  1.3737951486473294\n",
      "Epoch:  9500 G_loss:  0.6878457743619976 D_loss:  1.3954494073395831\n",
      "Epoch:  9600 G_loss:  0.7087219538186424 D_loss:  1.39576445499836\n",
      "Epoch:  9700 G_loss:  0.696123873018186 D_loss:  1.3897257216311052\n",
      "Epoch:  9800 G_loss:  0.6694974129026295 D_loss:  1.4260699156016896\n",
      "Epoch:  9900 G_loss:  0.7011926324238218 D_loss:  1.3825726606074151\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def generator(data, weight):\n",
    "    return np.dot(data, weight)\n",
    "\n",
    "def discriminator(data, weight):\n",
    "    return np.dot(data, weight)\n",
    "\n",
    "def train_gan(data, epochs, generator=generator, discriminator=discriminator):\n",
    "    learning_rate = 0.01\n",
    "    latent_dim = 100\n",
    "    data_dim = 2\n",
    "    num_samples =1000\n",
    "    batch_size = 32\n",
    "    eps = 1e-8\n",
    "    \n",
    "    weights_g = np.random.normal(size=(latent_dim, data_dim))\n",
    "    weights_d = np.random.normal(size=(data_dim, 1))\n",
    "    print(weights_g.shape)\n",
    "    for epoch in range(epochs):\n",
    "        idx = np.random.randint(0, num_samples, batch_size)\n",
    "        real_batch = data[idx]\n",
    "\n",
    "        z = np.random.normal(size=(batch_size, latent_dim))\n",
    "        fake_data = generator(z, weights_g)\n",
    "\n",
    "        d_real = discriminator(real_batch, weights_d)\n",
    "        d_fake = discriminator(fake_data, weights_d)\n",
    "\n",
    "        sig_d_real = sigmoid(d_real)\n",
    "        sig_d_fake = sigmoid(d_fake)\n",
    "\n",
    "        d_loss_real = -np.mean(np.log(np.clip(sig_d_real, eps, 1 - eps)))\n",
    "        d_loss_fake = -np.mean(np.log(np.clip(1 - sig_d_fake, eps, 1 - eps)))\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        grad_real = np.dot(real_batch.T, (sig_d_real - 1)) / batch_size\n",
    "        grad_fake = np.dot(fake_data.T, sig_d_fake) / batch_size\n",
    "        del_weights_d = grad_real + grad_fake\n",
    "        weights_d -= learning_rate * del_weights_d\n",
    "\n",
    "        g_loss = -np.mean(np.log(np.clip(sig_d_fake, eps, 1 - eps)))\n",
    "\n",
    "        grad_g = np.dot(z.T, sig_d_fake) / batch_size\n",
    "        weights_g -= learning_rate * grad_g\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch: \", epoch, \"G_loss: \", g_loss, \"D_loss: \", d_loss)\n",
    "    return weights_g, generator\n",
    "num_samples = 1000\n",
    "data_dim = 2\n",
    "data = np.random.normal(loc=0, scale=1.0, size=(num_samples, data_dim))\n",
    "data[:, 1] = -data[:, 0]\n",
    "weights_g , generator = train_gan(data, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "31b99b4b-4bda-4073-acc0-fec0963f4602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample_data:  [[-3.06214753  3.80400616]]\n"
     ]
    }
   ],
   "source": [
    "sample_data = generator(np.random.normal(size=(1, 100)), weights_g)\n",
    "print(\"Sample_data: \", sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d700d-4328-400f-8ae7-96405a66e5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
